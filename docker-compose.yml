services:

  
  api-gateway:  # Cambiato da "app" a "api-gateway"
    build:
      context: ./api-gateway  # Cartella di build per api-gateway
      dockerfile: ${BACKEND_DOCKERFILE_NAME}  # Usa il Dockerfile.dev per lo sviluppo
    container_name: api-gateway
    ports:
      - "8000:8000"
    depends_on:
      mongo: 
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    volumes:
      - ./api-gateway/nginx.conf:/etc/nginx/nginx.conf  # Monta il file di configurazione del backend
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - AWS_S3_BUCKET=${AWS_S3_BUCKET}
      - MONGO_URI=mongodb://mongo:27017/3dgs_models
      - RABBITMQ_HOSTNAME=rabbitmq  # HOST RabbitMQ
      - RABBITMQ_PORT=5672  # HOST RabbitMQ
      - RABBITMQ_USER=${RABBIT_MQ_USER_WRITER}
      - RABBITMQ_PASS=${RABBIT_MQ_PASS_WRITER}
    networks:
      - my_network

  job-executor:
    build:
      context: ./job-executor
      dockerfile: Dockerfile  # Dockerfile per job-executor
    container_name: job-executor
    depends_on:
      mongo: 
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    volumes:
      - shared_data:${MODEL_WORKING_DIR}   # Stesso volume condiviso
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - AWS_S3_BUCKET=${AWS_S3_BUCKET}
      - MONGO_URI=mongodb://mongo:27017/3dgs_models
      - RABBITMQ_HOSTNAME=rabbitmq  # HOST RabbitMQ
      - RABBITMQ_PORT=5672  # HOST RabbitMQ
      - RABBITMQ_USER=${RABBIT_MQ_USER_READER}
      - RABBITMQ_PASS=${RABBIT_MQ_PASS_READER}
      - MODEL_WORKING_DIR=${MODEL_WORKING_DIR}
      - NVIDIA_VISIBLE_DEVICES=all  # Rende visibili tutte le GPU
    restart: always
    networks:
      - my_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Usa una GPU
              capabilities: [gpu]


  gaussian-splatting:  # Cambiato da "app" a "api-gateway"
    build:
      context: ./gaussian-splatting  # Cartella di build per api-gateway
      dockerfile: Dockerfile.base  # Usa Dockerfile.base per costruire l'immagin
    container_name: gaussian-splatting
    image: gaussian-splatting  # ðŸ‘ˆ Aggiungi questa riga
    networks:
      - my_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Usa tutte le GPU disponibili
              capabilities: [gpu]

  gaussian-splatting-api:
    build:
      context: ./gaussian-splatting  # Nuovo servizio API per Gaussian Splatting
      dockerfile: Dockerfile
      args:
      - API_PORT=${GAUSSIAN_SPLATTING_PORT:-8100}
    container_name: gaussian-splatting-api
    mem_limit: 20G  # Imposta un limite di memoria piÃ¹ alto (ad esempio 32 GB)
    memswap_limit: 32G  # Limite di memoria + swap a 36 GB
    volumes:
      - shared_data:${MODEL_WORKING_DIR}   # Stesso volume condiviso
      - /tmp/.X11-unix:/tmp/.X11-unix  # Monta il socket X11
    ports:
      - "${GAUSSIAN_SPLATTING_PORT:-8100}:${GAUSSIAN_SPLATTING_PORT:-8100}"
    depends_on:
      - gaussian-splatting
    environment:
      - API_PORT=${GAUSSIAN_SPLATTING_PORT:-8100}
      - MODEL_WORKING_DIR=${MODEL_WORKING_DIR}
      - XDG_RUNTIME_DIR=/tmp/runtime-root
      - DISPLAY=${DISPLAY}  # Usa il DISPLAY del sistema host
    networks:
    - my_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Usa tutte le GPU disponibili
              capabilities: [gpu]

  3dgs-mcmc:  # Cambiato da "app" a "api-gateway"
    build:
      context: ./3dgs-mcmc  # Cartella di build per api-gateway
      dockerfile: Dockerfile.base  # Usa Dockerfile.base per costruire l'immagin
    container_name: 3dgs-mcmc
    image: 3dgs-mcmc  # ðŸ‘ˆ Aggiungi questa riga
    networks:
      - my_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Usa tutte le GPU disponibili
              capabilities: [gpu]

  3dgs-mcmc-api:
    build:
      context: ./3dgs-mcmc  # Nuovo servizio API per Gaussian Splatting
      dockerfile: Dockerfile
      args:
      - API_PORT=${MCMC_PORT:-8100}
    container_name: 3dgs-mcmc-api
    volumes:
      - shared_data:${MODEL_WORKING_DIR}   # Stesso volume condiviso
      - /tmp/.X11-unix:/tmp/.X11-unix  # Monta il socket X11
    ports:
      - "${MCMC_PORT:-8100}:${MCMC_PORT:-8100}"
    depends_on:
      - 3dgs-mcmc
    mem_limit: 20G
    memswap_limit: 32G  # PiÃ¹ swap per GPU intensive tasks
    mem_swappiness: 60  # Meno aggressivo per performance GPU
    environment:
      - API_PORT=${MCMC_PORT:-8101}
      - MODEL_WORKING_DIR=/app/shared_data
      - XDG_RUNTIME_DIR=/tmp/runtime-root
      - DISPLAY=${DISPLAY}  # Usa il DISPLAY del sistema host
    networks:
    - my_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Usa tutte le GPU disponibili
              capabilities: [gpu]

  taming-3dgs:  # Cambiato da "app" a "api-gateway"
    build:
      context: ./taming-3dgs  # Cartella di build per api-gateway
      dockerfile: Dockerfile.base  # Usa Dockerfile.base per costruire l'immagin
    container_name: taming-3dgs
    image: taming-3dgs  # ðŸ‘ˆ Aggiungi questa riga
    networks:
      - my_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Usa tutte le GPU disponibili
              capabilities: [gpu]

  taming-3dgs-api:
    build:
      context: ./taming-3dgs  # Nuovo servizio API per Gaussian Splatting
      dockerfile: Dockerfile
      args:
      - API_PORT=${TAMING_PORT:-8100}
    container_name: taming-3dgs-api
    
    mem_limit: 20G  # Imposta un limite di memoria piÃ¹ alto (ad esempio 32 GB)
    memswap_limit: 32G  # Limite di memoria + swap a 36 GB
    volumes:
      - shared_data:${MODEL_WORKING_DIR}   # Stesso volume condiviso
      - /tmp/.X11-unix:/tmp/.X11-unix  # Monta il socket X11
    ports:
    - "${TAMING_PORT:-8100}:${TAMING_PORT:-8100}"
    depends_on:
      - taming-3dgs
    environment:
      - API_PORT=${TAMING_PORT:-8102}
      - MODEL_WORKING_DIR=/app/shared_data
      - XDG_RUNTIME_DIR=/tmp/runtime-root
      - DISPLAY=${DISPLAY}  # Usa il DISPLAY del sistema host
    networks:
    - my_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Usa tutte le GPU disponibili
              capabilities: [gpu]

  colmap-converter-api:  # Cambiato da "app" a "api-gateway"
    build:
      context: ./colmap-converter  # Cartella di build per api-gateway
      dockerfile: Dockerfile  # Usa Dockerfile.base per costruire l'immagin
    container_name: colmap-converter-api
    volumes:
      - shared_data:${MODEL_WORKING_DIR}   # Stesso volume condiviso
      - /tmp/.X11-unix:/tmp/.X11-unix  # Monta il socket X11
    image: colmap-converter  # ðŸ‘ˆ Aggiungi questa riga
    ports:
      - "8060:8060"  # Espone l'API sulla porta 8060
    networks:
      - my_network
    environment:
      - XDG_RUNTIME_DIR=/tmp/runtime-root
      - DISPLAY=${DISPLAY}  # Usa il DISPLAY del sistema host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Usa tutte le GPU disponibili
              capabilities: [gpu]

  web-viewer:
    build:
      context: ./web-viewer
      dockerfile: ${FRONTEND_DOCKERFILE_NAME}  # Usa il Dockerfile.dev per lo sviluppo
    container_name: web-viewer
    restart: always
    ports:
      - "5173:5173"  # Ora il frontend Ã¨ pubblico su 8080
      - "8080:8080"  # Ora il frontend Ã¨ pubblico su 8080
    volumes:
      - ./web-viewer:/app  # Monta il codice locale dentro il container
      - /app/node_modules  # Evita di sovrascrivere i moduli Node.js
      - ./web-viewer/nginx.conf:/etc/nginx/nginx.conf  # Monta il file di configurazione del frontend
    depends_on:
      - api-gateway
    environment:
      - CHOKIDAR_USEPOLLING=true  # Abilita hot reload su Docker
    networks:
      - my_network
      
  mongo:
    image: mongo
    container_name: mongo
    command: mongod --replSet rs0 --bind_ip_all
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db  # Aggiungi il volume persistente per MongoDB
    networks:
      - my_network
    environment:
      - MONGO_INITDB_DATABASE=3d_gaussian_splatting
    
  # Servizio separato per inizializzare il replica set
  mongo-setup:
    image: mongo:5.0
    container_name: mongo-setup
    depends_on:
      mongo:
        condition: service_healthy  # Aspetta che mongo sia healthy
    networks:
      - my_network
    command: |
      mongosh --host mongo:27017 --eval "
      try {
        rs.status();
        print('Replica set already initialized');
      } catch(e) {
        rs.initiate({
          _id: 'rs0',
          members: [{ _id: 0, host: 'mongo:27017' }]
        });
        print('Replica set initialized');
      }"
    restart: "no"

  rabbitmq:
    image: "rabbitmq:3-management"  # Usa l'immagine RabbitMQ con interfaccia di gestione
    container_name: rabbitmq
    hostname: 3dgs-job-queue    
    ports:
      - "15672:15672"  # Interfaccia di gestione RabbitMQ
      - "5672:5672"    # Porta di comunicazione AMQP (RabbitMQ)
    healthcheck:
      test: ["CMD", "rabbitmqctl", "status"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq/mnesia/  # Aggiungi il volume persistente per Rabbitmq
      - ./rabbitmq/definitions.json:/etc/rabbitmq/definitions.json  # Monta il file definitions.json direttamente
    networks:
      - my_network
    environment:
      - RABBITMQ_DEFAULT_USER=${RABBIT_MQ_DEFAULT_USER}
      - RABBITMQ_DEFAULT_PASS=${RABBIT_MQ_DEFAULT_PASS}
    command: >
      bash -c "rabbitmq-server & 
               until rabbitmqctl await_startup; do sleep 1; done; 
               rabbitmqctl import_definitions /etc/rabbitmq/definitions.json; 
               tail -f /dev/null"

volumes:
  mongodb_data:  # Definisce il volume del db
    driver: local
  rabbitmq_data:  # Definisce il volume della coda di messaggi
    driver: local 
  shared_data:  # Definisce il volume condiviso tra job-executor e gaussian-splatting
    driver: local

networks:
  my_network:
    driver: bridge


